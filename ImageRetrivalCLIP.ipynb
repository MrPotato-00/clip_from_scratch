{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b183abed-df27-423b-a31e-a5484c3498e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6386135c-2fc4-4453-a0da-74d26e24d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text encoding\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.embedding= nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class ImagePatching(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        super(ImagePatching, self).__init__()\n",
    "        self.projection= nn.Conv2d(in_channels, embed_dim, kernel_size= patch_size, stride= patch_size)\n",
    "    \n",
    "    ## what transformation happens here:\n",
    "    ## lets say we pass an input image batch of shape: [batch_size, input_channels, Height, Width]\n",
    "    ## then finally transforms it into: [batch_size, (image_size//patch_size)**2, d_model]\n",
    "    def forward(self, x): \n",
    "        x= self.projection(x) \n",
    "        x= x.flatten(2)\n",
    "        return x.transpose(-2, -1) \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe= torch.zeros(max_len, d_model)\n",
    "\n",
    "        for i in range(max_len):\n",
    "            for j in range(d_model):\n",
    "                if j%2==0:\n",
    "                    pe[i][j]= np.sin(i/ (10000** (j/d_model)))\n",
    "                else:\n",
    "                    pe[i][j]= np.cos(i/(10000** ((j-1)/d_model)))\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x= x+ self.pe\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f96245c-949b-42b3-aedb-9a9d71e24e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_size):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.head_size= head_size\n",
    "        self.query= nn.Linear(embed_dim, head_size)\n",
    "        self.key= nn.Linear(embed_dim, head_size)\n",
    "        self.value= nn.Linear(embed_dim, head_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q= self.query(x)\n",
    "        K= self.key(x)\n",
    "        V= self.value(x)\n",
    "\n",
    "        attention= Q@K.transpose(-2, -1)\n",
    "        attention= attention/(self.head_size**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention= attention.masked_fill(mask==0, float(\"-inf\"))\n",
    "        attention= torch.softmax(attention, dim=-1)\n",
    "\n",
    "        attention= attention@V\n",
    "        return attention\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79323de-bdb5-46ab-b771-1cdafb654c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, nheads):\n",
    "        super().__init__()\n",
    "        self.head_size= embed_dim//nheads\n",
    "        self.W_o= nn.Linear(embed_dim, embed_dim)\n",
    "        self.attentionblocks= nn.ModuleList([AttentionHead(embed_dim, self.head_size) for _ in range(nheads)])\n",
    "\n",
    "    def forward(self, x, mask= None):\n",
    "        out= torch.cat([head(x, mask) for head in self.attentionblocks] ,dim =-1)\n",
    "        out= self.W_o(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb296a86-d545-459c-8477-b8f6584fddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nheads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.norm1= nn.LayerNorm(d_model)\n",
    "        self.mha= MultiHeadAttention(d_model, nheads)\n",
    "        self.mlp= nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model* r_mlp, d_model)\n",
    "        )\n",
    "        self.norm2= nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x= x+ self.mha(self.norm1(x), mask)\n",
    "        x= x+ self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ed025b-a1a2-47ac-9d26-84eec541f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, width, image_size, patch_size, in_channels, n_layers,nheads, embed_dim, dropout=0.1):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        assert (image_size[0] % patch_size[0]==0) and (image_size[1] % patch_size[1]==0), \"image_size dim shall be divisible by patchsize\"\n",
    "        assert width%nheads==0, \"width shall be divisible by nheads\"\n",
    "        \n",
    "        self.image_embedding= ImagePatching(image_size, patch_size, in_channels, width)\n",
    "        self.npatches= (image_size[0]*image_size[1]) // (patch_size[0]*patch_size[1])\n",
    "        self.cls_token= nn.Parameter(torch.randn(1,1, width))\n",
    "\n",
    "        self.positional_embedding= PositionalEncoding(width, self.npatches+1, dropout)\n",
    "        self.encoder= nn.ModuleList([TransformerEncoder(width, nheads) for _ in range(n_layers)])\n",
    "\n",
    "        self.projection= nn.Parameter(torch.randn(width ,embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.image_embedding(x)\n",
    "        #print(x.shape, self.cls_token.shape)\n",
    "        x= torch.cat([self.cls_token.expand(x.size()[0], -1, -1), x], dim=1)\n",
    "        x= self.positional_embedding(x)\n",
    "\n",
    "        for encoder in self.encoder:\n",
    "            x= encoder(x)\n",
    "\n",
    "        x= x[:, 0, :]\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x= x@self.projection\n",
    "\n",
    "        x= x/torch.norm(x, dim=-1, keepdim=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "182ac609-974a-4a88-a307-56abc60c5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, width, max_seq_length, n_heads, n_layers, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, width)\n",
    "\n",
    "        self.positional_embedding = PositionalEncoding(width, max_seq_length, 0.1)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])\n",
    "\n",
    "        # learned proj of image to embed\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "    def forward(self, text, mask=None):\n",
    "        # Text Embedding\n",
    "        x = self.encoder_embedding(text)\n",
    "\n",
    "        # Positional Embedding\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x, mask=mask)\n",
    "\n",
    "        # Takes features from the EOT Embedding\n",
    "        x = x[torch.arange(text.shape[0]), torch.sub(torch.sum(mask[:,0],dim=1),1)]\n",
    "\n",
    "        # joint multimodal embedding\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70e597c-bd18-4ca2-97b0-39f4aa7edd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54c6b1aa-202e-41d2-98b8-4af43b860df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self, emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = ImageEncoder(vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, emb_dim)\n",
    "        \n",
    "        self.text_encoder = TextEncoder(vocab_size, text_width, max_seq_length, text_heads, text_layers, emb_dim)\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n",
    "        self.device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, image,text, mask=None):\n",
    "        i_encode= self.image_encoder(image)\n",
    "        t_encode= self.text_encoder(text, mask)\n",
    "        #print(i_encode.shape, t_encode.shape)\n",
    "        logits= (i_encode @ t_encode.T)/ torch.exp(self.temperature)\n",
    "        #labels= torch.arange(logits.shape[0]).to(self.device)\n",
    "\n",
    "        return logits\n",
    "\n",
    "        #loss_i= nn.functional.cross_entropy(logits, labels)\n",
    "        #loss_t= nn.functional.cross_entropy(logits.transpose(-2, -1), labels)\n",
    "\n",
    "        #loss= (loss_i+loss_t)/2\n",
    "        #return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa8dd6eb-93db-456a-8c19-c8d50ade72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, encode=True, mask=None, max_seq_length=32):\n",
    "    if encode:\n",
    "        out = chr(2) + text + chr(3) # Adding SOT and EOT tokens\n",
    "        out = out + \"\".join([chr(0) for _ in range(max_seq_length-len(out))]) # Adding Padding\n",
    "        out = torch.IntTensor(list(out.encode(\"utf-8\"))) # Encoding Text\n",
    "        mask = torch.ones(len(out.nonzero()))\n",
    "        mask = torch.cat((mask,torch.zeros(max_seq_length-len(mask)))).type(torch.IntTensor)\n",
    "    else:\n",
    "        out = [chr(x) for x in text[1:len(mask.nonzero())-1]]\n",
    "        out = \"\".join(out)\n",
    "        mask = None\n",
    "\n",
    "    return out, mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c50f64b-9419-4a70-a51b-0a5d92d6d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDS(Dataset):\n",
    "    def __init__(self, img_size, train=True):\n",
    "        #self.dataset = load_dataset(\"fashion_mnist\")\n",
    "        \n",
    "        self.transform= T.Compose([T.Resize(img_size), \n",
    "                                   T.ToTensor()\n",
    "                        ])\n",
    "        self.dataset= FashionMNIST(\n",
    "            root='./data', \n",
    "            train=train, \n",
    "            download=True\n",
    "        )\n",
    "        if train:\n",
    "            self.split=\"train\"\n",
    "        else:\n",
    "            self.split= \"test\"\n",
    "\n",
    "        self.captions = {0: \"An image of a t-shirt/top\",\n",
    "                        1: \"An image of trousers\",\n",
    "                        2: \"An image of a pullover\",\n",
    "                        3: \"An image of a dress\",\n",
    "                        4: \"An image of a coat\",\n",
    "                        5: \"An image of a sandal\",\n",
    "                        6: \"An image of a shirt\",\n",
    "                        7: \"An image of a sneaker\",\n",
    "                        8: \"An image of a bag\",\n",
    "                        9: \"An image of an ankle boot\"}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img= self.dataset[idx][0]\n",
    "        \n",
    "        img= self.transform(img)\n",
    "\n",
    "        cap, mask= tokenizer(self.captions[self.dataset[idx][1]])\n",
    "        mask= mask.repeat(len(mask), 1)\n",
    "\n",
    "        return {\"image\": img, \"caption\": cap, \"mask\":mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2c88098-8c24-4923-aac0-2d55bf6a797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 32\n",
    "vit_width = 9\n",
    "img_size = (28,28)\n",
    "patch_size = (14,14)\n",
    "n_channels = 1\n",
    "vit_layers = 3\n",
    "vit_heads = 3\n",
    "vocab_size = 256\n",
    "text_width = 32\n",
    "max_seq_length = 32\n",
    "text_heads = 8\n",
    "text_layers = 4\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ae3e4f3-b97d-46b2-b3ba-4e0883e4764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set= PrepareDS(img_size, train=True)\n",
    "test_set= PrepareDS(img_size, train=False)\n",
    "\n",
    "train_dataloader= DataLoader(train_set, shuffle=True, batch_size= batch_size)\n",
    "test_dataloader= DataLoader(test_set, shuffle=False, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0b4eb77-49d8-4c83-bc5c-5badffe6b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce GTX 1650)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:27<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch Loss: 4.453\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:28<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Batch Loss: 4.366\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:29<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Batch Loss: 4.132\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:29<00:00, 15.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Batch Loss: 3.775\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:29<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Batch Loss: 3.362\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:29<00:00, 15.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Batch Loss: 2.855\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:29<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Batch Loss: 2.773\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:29<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Batch Loss: 2.772\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:29<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Batch Loss: 2.584\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 469/469 [00:30<00:00, 15.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Batch Loss: 2.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for data in tqdm(train_dataloader):\n",
    "        img, cap, mask = data[\"image\"].to(device), data[\"caption\"].to(device), data[\"mask\"].to(device)\n",
    "        logits= model(img,cap,mask)\n",
    "        #labels_image= torch.arange(logits.shape[0]).to(device)\n",
    "        labels_text= torch.arange(logits.shape[1]).to(device)\n",
    "        image_to_text= nn.functional.cross_entropy(logits, labels_text)\n",
    "        text_to_image= nn.functional.cross_entropy(logits.transpose(-2, -1), labels_text)\n",
    "        loss= (image_to_text + text_to_image)/2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.3f}\")\n",
    "\n",
    "    # Saves model if it performed better than the previous best\n",
    "    if loss.item() <= best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), \"clip.pt\")\n",
    "        print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "fab005c0-2a95-4569-99e1-71b4f1f41b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4, 32])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch= next(iter(train_dataloader))\n",
    "\n",
    "s= batch[\"image\"]\n",
    "image_embedding= ImageEmbedding(1, (28, 28),(14, 14), 32)\n",
    "image_= image_embedding(s)\n",
    "image_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e455a374-5e34-4098-b18d-7f02091cce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_set[0][\"index\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c2857f02-1352-4497-af26-d464f3cc1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token= torch.cat([torch.randn(1,1,32).expand(image_.size()[0], -1, -1),image_], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "61bd5513-cd73-4fec-834e-a6000fcaf207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 5, 32])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a737696-dc3e-4149-8639-25617d490ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0405d02f-f858-4acf-9b18-2c4e7df3dfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.tensor([[1, 1, 0], \n",
    "                 [1, 2, 1]])\n",
    "\n",
    "b= torch.tensor([[0, 2, 1], \n",
    "                 [1, 1, 1]])\n",
    "\n",
    "torch.eq(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb60d4cc-f397-46c6-b0bf-7ce72f7ba58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                          | 4/313 [00:00<00:19, 16.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█                                          | 8/313 [00:00<00:17, 17.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                        | 12/313 [00:00<00:16, 18.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                       | 16/313 [00:00<00:16, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▋                                       | 20/313 [00:01<00:15, 18.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                      | 24/313 [00:01<00:15, 18.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▊                                      | 28/313 [00:01<00:15, 18.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▎                                     | 32/313 [00:01<00:15, 18.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▊                                     | 36/313 [00:01<00:14, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▎                                    | 40/313 [00:02<00:14, 18.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▉                                    | 44/313 [00:02<00:14, 18.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▍                                   | 48/313 [00:02<00:14, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▉                                   | 52/313 [00:02<00:14, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▌                                  | 56/313 [00:03<00:13, 18.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████                                  | 60/313 [00:03<00:13, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▌                                 | 64/313 [00:03<00:13, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████                                 | 68/313 [00:03<00:13, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████▋                                | 72/313 [00:03<00:12, 18.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████▏                               | 76/313 [00:04<00:12, 18.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▋                               | 80/313 [00:04<00:12, 18.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████▎                              | 84/313 [00:04<00:12, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████▊                              | 88/313 [00:04<00:12, 18.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▎                             | 92/313 [00:04<00:11, 18.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████▉                             | 96/313 [00:05<00:11, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████                            | 100/313 [00:05<00:11, 18.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████▌                           | 104/313 [00:05<00:11, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████▏                          | 108/313 [00:05<00:11, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████▋                          | 112/313 [00:06<00:10, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███████████████▏                         | 116/313 [00:06<00:10, 18.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████▋                         | 120/313 [00:06<00:10, 18.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████▏                        | 124/313 [00:06<00:10, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████████████████▊                        | 128/313 [00:06<00:09, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████▎                       | 132/313 [00:07<00:09, 18.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████▊                       | 136/313 [00:07<00:09, 18.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████▎                      | 140/313 [00:07<00:09, 18.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████▊                      | 144/313 [00:07<00:09, 18.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████▍                     | 148/313 [00:07<00:08, 18.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████▉                     | 152/313 [00:08<00:08, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████▍                    | 156/313 [00:08<00:08, 18.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████▉                    | 160/313 [00:08<00:08, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████▍                   | 164/313 [00:08<00:07, 18.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|██████████████████████                   | 168/313 [00:09<00:07, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████▌                  | 172/313 [00:09<00:07, 18.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████                  | 176/313 [00:09<00:07, 18.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████▌                 | 180/313 [00:09<00:07, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████                 | 184/313 [00:09<00:06, 18.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████▋                | 188/313 [00:10<00:06, 18.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████▏               | 192/313 [00:10<00:06, 18.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████▋               | 196/313 [00:10<00:06, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████▏              | 200/313 [00:10<00:06, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████▋              | 204/313 [00:10<00:05, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████▏             | 208/313 [00:11<00:05, 18.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████▊             | 212/313 [00:11<00:05, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████▎            | 216/313 [00:11<00:05, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████▊            | 220/313 [00:11<00:04, 18.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████▎           | 224/313 [00:12<00:04, 18.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████▊           | 228/313 [00:12<00:04, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████▍          | 232/313 [00:12<00:04, 18.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████▉          | 236/313 [00:12<00:04, 18.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████▍         | 240/313 [00:12<00:03, 18.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████▉         | 244/313 [00:13<00:03, 18.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████▍        | 248/313 [00:13<00:03, 18.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████        | 252/313 [00:13<00:03, 18.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████▌       | 256/313 [00:13<00:03, 18.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████       | 260/313 [00:13<00:02, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|██████████████████████████████████▌      | 264/313 [00:14<00:02, 18.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████      | 268/313 [00:14<00:02, 18.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████▋     | 272/313 [00:14<00:02, 18.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████▏    | 276/313 [00:14<00:02, 18.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████████████████████████████████▋    | 280/313 [00:15<00:01, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████████▏   | 284/313 [00:15<00:01, 18.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████▋   | 288/313 [00:15<00:01, 18.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|██████████████████████████████████████▏  | 292/313 [00:15<00:01, 18.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████▊  | 296/313 [00:15<00:00, 18.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████▎ | 300/313 [00:16<00:00, 18.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████▊ | 304/313 [00:16<00:00, 18.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████▎| 308/313 [00:16<00:00, 18.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████▊| 312/313 [00:16<00:00, 18.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n",
      "Pred and label: torch.Size([32, 32]), torch.Size([32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [00:16<00:00, 18.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred and label: torch.Size([16, 32]), torch.Size([16, 32])\n",
      "\n",
      "Model Accuracy: 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## for testing purpose\n",
    "#text= torch.stack([tokenizer(x)[0] for x in test_set.captions.values()]).to(device) ## [10, 32]\n",
    "#mask= torch.stack([tokenizer(x)[1] for x in test_set.captions.values()]).to(device)\n",
    "#mask= mask.repeat(1, len(mask[0])).reshape(len(mask), len(mask[0]),len(mask[0])).to(device)\n",
    "\n",
    "#image= torch.stack([x for x in test_set.image.values]\n",
    "##print(text.shape, mask.shape)\n",
    "image_stack= torch.stack([image[\"image\"] for image in test_set]).to(device)\n",
    "correct, total= 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_dataloader):\n",
    "        _,text, mask= data[\"image\"].to(device), data[\"caption\"].to(device), data[\"mask\"].to(device)\n",
    "        \n",
    "        image_features= model.image_encoder(image_stack) ## convert to shape: [batch, emb_dim] ==> [32, 32]\n",
    "        text_features= model.text_encoder(text, mask) ## convert to shape: [len(test_set), emb_dim] ==> [32, 32]\n",
    "\n",
    "        \n",
    "        ##image_features /= image_features.norm(dim=-1, keepdim=True) ## [10000,32]\n",
    "        ##text_features /= text_features.norm(dim=-1, keepdim=True) ## [32,32]\n",
    "\n",
    "        \n",
    "        similarity= (100* text_features @ image_features.T).softmax(dim=-1) ## [32, 10000] \n",
    "        #print(similarity.shape)\n",
    "        _, indices= torch.max(similarity, 1) ## [32]\n",
    "        \n",
    "        pred= torch.stack([test_set[int(i)][\"caption\"] for i in indices]).to(device) ## [32, 32]\n",
    "        #print(pred.shape)\n",
    "        print(f\"Pred and label: {pred.shape}, {text.shape}\") \n",
    "        correct+= int(sum(torch.sum((pred==text), dim=1)//len(pred[0])))\n",
    "        \n",
    "        total+= len(text)\n",
    "\n",
    "    print(f\"\\nModel Accuracy: {100* correct//total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "8660b593-c242-4c34-a60d-0d8e8b7166af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0][\"image\"].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3ac8bed9-1bba-4eb7-84aa-b6561c15db6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.tensor([1, 0, 1]) \n",
    "b= torch.tensor([1, 0, 0])\n",
    "\n",
    "torch.sum(a==b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c8d57-7e14-43ef-85d1-ba0fa52b9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.tensor([1, 1,1])\n",
    "b= torch.tensor([1, 0, 1])\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "sum(torch.sum(a==b, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "3c2a8805-08ab-474e-b09b-cb8826fbcfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  2, 115, 104, 105, 114, 116,   3,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0], dtype=torch.int32),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32))"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_= \"shirt\"\n",
    "tokens, mask= tokenizer(text_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "b23392d9-a0da-44c5-90bb-c9d2237f53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "29b2f128-0e0b-4b41-98ba-d3d2a6bc872f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text_\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshirt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m mask\u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m text_, mask\u001b[38;5;241m=\u001b[39m tokenizer(text_)\n\u001b[1;32m      4\u001b[0m mask\u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor"
     ]
    }
   ],
   "source": [
    "text_= \"shirt\"\n",
    "mask= mask.repeat(len(mask), 1)\n",
    "text_, mask= tokenizer(text_)\n",
    "mask= mask.unsqueeze(0)\n",
    "print(mask.shape)\n",
    "mask= mask.repeat(len(mask), 1)\n",
    "print(text_.unsqueeze(0).shape)\n",
    "print(mask.shape)\n",
    "text__= model.text_encoder(text__.unsqueeze(0).to(device), mask= mask.unsqueeze(0).to(device))\n",
    "text__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c6736-4a24-472b-8929-8a3ccbc597d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_= mask.unsqueeze(0).to(device)\n",
    "print(mask_.shape)\n",
    "torch.sum(mask_[:, 0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f83740e-ac57-4566-9f7e-b8157663e19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32]) torch.Size([32, 32, 32])\n",
      "torch.Size([32, 32])\n"
     ]
    }
   ],
   "source": [
    "batch= next(iter(test_dataloader))\n",
    "\n",
    "text= batch[\"caption\"]\n",
    "mask= batch[\"mask\"]\n",
    "#text_, mask_= tokenizer(text)\n",
    "print(text.shape, mask.shape)\n",
    "\n",
    "text_= model.text_encoder(text.to(device), mask= mask.to(device))\n",
    "print(text_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff3c576d-b0ad-413a-a9ea-22dcf3b15fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9943/1390394696.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"clip.pt\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
    "model.load_state_dict(torch.load(\"clip.pt\", map_location=device))\n",
    "def retrieve_images(query):\n",
    "    query_text, query_mask = tokenizer(query)\n",
    "    query_text = query_text.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    query_mask = query_mask.unsqueeze(0)\n",
    "    query_mask= query_mask.repeat(len(query_mask[0]), 1).unsqueeze(0).to(device)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_features = model.text_encoder(query_text, mask=query_mask)\n",
    "        #query_features /= query_features.norm(dim=-1, keepdim=True)\n",
    " \n",
    "    # Step 2: Encode all images in the dataset and store features\n",
    "    image_features_list = []\n",
    "    image_paths = []\n",
    " \n",
    "    #val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=5)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            features = model.image_encoder(images)\n",
    "            #features /= features.norm(dim=-1, keepdim=True)\n",
    " \n",
    "            image_features_list.append(features)\n",
    "            # Assuming batch contains image paths or IDs\n",
    " \n",
    "    # Concatenate all image features\n",
    "    image_features = torch.cat(image_features_list, dim=0)\n",
    " \n",
    "    # Step 3: Compute similarity using the CLIP model's logic\n",
    "    similarities = (query_features @ image_features.T) * torch.exp(\n",
    "        model.temperature\n",
    "    )\n",
    "    similarities = similarities.softmax(dim=-1)\n",
    " \n",
    "    # Retrieve top 20 matches\n",
    "    _, top_indices = similarities[0].topk(5)\n",
    "    print(type(top_indices))\n",
    "    print(top_indices)\n",
    "    # Step 4: Retrieve and display top N images\n",
    "    return top_indices\n",
    "    images_to_display = []\n",
    "    for index in top_indices:\n",
    "        img_path = test_set[int(index)][\"image\"]\n",
    "        print(f\"Type: {type(img_path)}\")\n",
    "        out_img2= img_path.detach().numpy()\n",
    "        arr_ = out_img2.squeeze\n",
    "        plt.imshow(arr_)\n",
    "        plt.show()\n",
    "        #images_to_display.append(np.array(img))\n",
    "        #img.show()\n",
    "        #plt.axis(\"off\")\n",
    "        #plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e910011b-1fb9-4dbd-92e0-5bd0148e7ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([ 612, 2986,  926, 8197, 5135], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "res= retrieve_images(\"shirt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4c1f35a6-c7bd-4f95-a469-c2ed9385d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_set)):\n",
    "    print(type(test_set[i][\"image\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ab3d5d69-2f79-487e-8eab-4e8c9df31d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6120, 4251, 1357, 2059,  684], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec62ea0d-cd32-47bc-82fa-a6269a2ccd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAQSUlEQVR4nO3ca6zfhV3H8e+5/M85bQ/taXvaAm1HKVBAnB2kQ1oNLnIzE+YMIIvojFNCWOYu0WVGTZY98DI34x4sG15G4uKMC6h7YFIccwOK2BEuA9cWWmgLlNIrvRxO23P7/31g8k18xPn+XM+OPa/XYz75nVt5/39Pvl2dTqcTABAR3T/uLwCA2UMUAEiiAEASBQCSKACQRAGAJAoAJFEAIPVO9z+8qfvOs/l18CMy9b5ryptDnzxd3lzwwR3lTWPdPfVNe2r2PmcG9axYXt68/KULypu1fz5Z3nSe21be8H/zSPvBd/xvvCkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACBN+yDeOWmGDqB1Nq0vbz7x9W+WNxERH3t0Q3nTc6q/vBl9eG15M3jbvvImIqIzMd5oVzbLj9uduPu68mbgNw6UNxN7+8qb/Z+t/44+ctnx8iYiYvNVQ412TI83BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApDl9EK+rp34Qr9PgaFrvK2+WN6+PLy1vIiIWLDld3kxsX1jevPF2q7xZ8M155U1ExMo7XqyPZvFxu6P3bGy0W3TXG+XN/idWlTe9A53y5kPXPVvePPT6NeVNRMSC2F3edLXqR/5m7BDjLONNAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASHP6SmpnamYuaU4dPFTefP6J9zd61o3rt5c3/zVwQXlz7Nll5c3gqrHyJiJi59feU95c+em95c3UkaPlzZlbry1vxn/xeHkTEbHnxfrvqdVbv3h69c/uLG+eOramvOm+f7i8+R/1K6kz9W/9XOBNAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAaU4fxIv27D2S1Xu82a/m+2++q7y59aJt5c0zf/RyefPGZzaVNxERvUvrR912/NnF5c3AvsvLm053/WsbPzGvvImIWPps/TPckWvrf+PD/aPlzb+9eGV5s27PSHkTEdFutGK6vCkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACDN7YN4s1hrbbNjYR9d93h585Wd15c3g3ctKm9OrWx2gHBozfHy5oqlh8qbD17/bHnzB0//cnnT+2qzg3iT87vKm5VrjjR6VtWf/vQ/lzdfe75+tLCxWXz8crbxpgBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOQgXlV3T33T4BjX5K7z6s+JiK/01o/bxZbF5Un3bx4ob24dfr28iYjY/J0N5c1v3flQefNPb723vJk6Vf8n9NCHvlTeRET87it3ljdvPraqvNm8cri8eXz5JeXNythW3nD2eVMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEDq6nQ6nen8hzd1149x0dzO+69tNuxtlyfzXu0rbybnT+vP5n/pGesqbyIilv6wflDw8Hvqn3faDc5D9p2of08Db9V/dhERb103Xt70zZ8ob7p3DJY3V9+0o7w5cUd/eRMRMflm/RjjTB2ynO0eaT/4jv+NNwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACA1uAt57uhq1a+Ddibqlyqb+Lub/7rR7p5/uK+8abfqVzu768c3o+d0fRMRcXBD/bPL5HD9Cxw+/2R5M/a94fLm5MXlSUREDG+p/72eXFvfdDX43a6cd7y82XH7xvqDImL5l+tXUrt66ldSO+fgldTp8KYAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYA0pw/izZTetWvKm+1nDjd61sTqsfKmM9rgz6DBx4mJ080+g7TntcubgX31Q3BL146WN68sW1reTM2vfz8REUc2NTjQNln/mfcfqh+PG53qL2+mbjxW3kRExJfrk65W/W98po5fzjbeFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkOb0QbzOVIMDYw1M7t5b3jx+bF2jZ/3ERW+WN9u2r64/qLd+1K3T6qo/JyJai+pH/iaP14+6DfRM1p+zon40rWuk2T+7xctHypvxyfqz5q2aKG9uHvphebP1gavLm6Y6E/Xf7VzlTQGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAGlOH8SLdoODeN31Q2tNnrP1uWYH8W7f9FR5sy3qB/H6zqsfghuPvvKmqXZ/p7x54fk15U1r2ZnyZup4q7yJiJhs1z/DzeurH7db0Ff/3e4aW1HerPj7+hG9iIj6KcaZO355LvCmAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApLl9JXUWG9zT4BprRMSm+qR7rP7ZoL+/fn2z0+4qbyIiJk70lzddS+qXPvterT9ncrj+PbWH6j+7iIixsfp11fkNrqQO9NY3e04vK2/aIyPlTWNNLiLPUd4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQHMSrmqHDWhd+90Sz4d31ybLLj5Q3vd3t8ubtE/PKm6ZaA5PlzeSl9e+p9+X69zR1fv1ri4iYHK8fSZxq1z/3Lew7U96s6j9W3rwSM/f3wPR5UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQHIQr6q7fpSsyRG9nhOj9edExPGJ+eXN+GSDQ2vd9c8Tg4tOlzcREW83OJw2ta/+c7j06tfLm30DQ+VNX4NjghERY2Ot8mbhQP243aJWffMvr60vb5bEzvKGs8+bAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkoN4s9Xp+lGyiIh5PePlzfy+ifLm+Kn6kbqmhhbXjwOOHOwvb15+bnV5s2nT9vJmx9Hzy5umlg7Uf3Zj7fqBxNPj9WN9zE7eFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkBzEq2pPzchjOhP1I3UREb+3/HvlzcdHby9vDh07r7yZHK8fWouIWLX6eHnT95P139Phl4bLmy3PXVHerL/q1fImIuLlo/Wv79Cp+u/pskWHy5vRt2buQCJnlzcFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAguZJa1d3g0meDy6pTl66sPyci9k/Wr1W+sLf+rBuueKm8eWz3peVNRMTLB5aVN5eeX7/0ueDd+8ubY6fqP++Rif7yJiJicGCsvDl5pv6sx47Wf0+Ll4+UN8xO3hQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJAcxKtqcNyuiV2/Vj+0FhHxq1vuKW9aA5PlzQ8ONzjYt3d+fRMRXQ1uEO4bGCpvfmpF/SDe7t0rypv53x4qbyIiDt02Ud58fMN3y5t//JNfKG+u/dT28uapu68rbyIiFn1ja6Md0+NNAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAyUG8GXDk3o3lzQ3vfaHRs3aPLC1vTp4ZKG+O7F9U3tz4882+pybH9y4eOlreXL94Z3mzbfuV5c3IqvIkIiIWP1n/DPfkJZeUN8c+cKq8efiRDeXNdR/bVt5ERBz5zvLyZurgoUbPmou8KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIDmIV9TV6itvWr90uLx55mCzq2mfXPfv5c1f3P8r5c3C8iLib279jwariA+3ry9vTo7PK29eG6sfE+wZ65Q3Z4a7ypuIiHarvtvzwLr6g24ZLU9aJ+tf25N71pY3ERGdL9afdemvO4g3Xd4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQHMQreu33N5Q3vfUbdXHL3f9ZH0XE5zbfUd6s3DtZ3hy7rP6n863RwfImImLr3ovLm6nJ+uedHX0ryptFp8uTiHaDTUSMXjRV3gzuqx+PW7bo7fLm1Max8mbx14fKm4iId31qZ3mz6yMby5slDzT7N/j/nTcFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgzekrqb2rV5U3py8arz+oXb9UedvQc/XnRMTWLdeWNyfW1P8MxpZ0yps1vUfLm4iI5YtHypv+3vrl18sXHSpvHrvgmvKmNVqeRETE+Fj97+jkmvrnvp5Hzi9v/vKjf1XefOKSe8ubiIj9z6wrbx7+7BfLm9954GfKm3OBNwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKQ5fRBvz4ffVd70Hagfglt13Rvlzed2f6C8iYgYW1jv/KkL69/Tgn3142x7J5eWNxERg31j5c27h/aXN4t7T5U3nRn8WDV/f/1h7Qb/wpt8T5/edkd5M3pZg+OSETGwr6+8+cKBm8ubnX97ZXmz7refLm9mG28KACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABI58xBvCP3bixv1t60p7zZu/ni8ubN4wvLm9YT9U1ERHtZfdN/tL45M1w/ordrbEX9QRGx++BweXPnBc+UN3/81PvLm8Ez5UmcXDdZH0VEzJuqb073lCcLdzb438LmJeXJVXftrT8nInaMXFTePPmt9eXNyp87UN5Mve+a8iYioufRZxvtzgZvCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASOfMQbyxoa7y5sVn6oe1uhbXD8H1d9c3py6sbyIi2q36rr2gfmita7z+eeLYxILyJiJi+eKR8ubzP7ilvFnyRH95M3Lj2+VN76vNfg5DT9eP2w3urx/fe+2WdnmzYF/972HXwQbXGyMihsfKk9aeeeXNqfFWebP/jvomIuKyRxvNzgpvCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQOrqdDrTOqt5U/edZ/trmXGv/+Gm+qhBRtd89aXyZu+9l9cfFBE94/XNqm8fK2/az+8ob7rXX1neRES8eN95jXYzYcWW+h/Eom9sPQtfyY9O7+pV5c3hG1aXN8PfP1LeRERM7dhV3rzxmfq/9aXb6hdmB/71qfJmJj3SfvAd/xtvCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASHP6IF5Xq6+86Uw0uDjHjDt838byZmKwq7y58AtPljfMvO7z6kcV2yMjZ+Er+fFyEA+AElEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEjTPogHwLnPmwIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIA6b8BqM/mtWomf9cAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAM30lEQVR4nO3cS6ik+VnH8X9VdZ1z+jY9GeaWi4PXBIMX8BI1G91EI4Iyi4AGQZBAMAs36iYK7lRcKQEjboI6XiBbUXCyyUJNIqJOgoY4kmTIZG6Z7unudPfpU6fechH4gaDkPM90v3Po/nzW/eOtrnN6vlWLeRa73W43AGCMsXyjXwAAp4coABCiAECIAgAhCgCEKAAQogBAiAIAceakf/A9y/fdzdfBHfKVD7+7vDk+V///F7/1t/6pvJnVYlHf+P84xxhjfPU36r9D51+YyptLT32qvOH1eXr6+Df9M74pABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMSJD+Ixr6998Mdau7/54O+XN5+89e3lzV88/TPlzfKT/1refGO46u2qdtt5njOjzU/+UHnzmx/4q/JmuagfxPvYMz9d3owxxvTM51s7TsY3BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwEO+UuvDV3nG2/9g8XN58+XZ988d/9pHy5ld//P3lzRhjHH/pudbuXrN6x3eWN9/9u/9W3jxz81vKm7effbG8WRxuyhvuPt8UAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhXUk+pvStHrd35RX03jUV58zsvvLe8+dAn/r68GWOMX3/ql8ubb/vos+XN9Orl8mb11jeXN//5a28pb8YY4/fe+9flzd9d/r7y5tx+/XfokTPXypux8pn0NPJTASBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgH8U6p9as3WrvlYrrDr+T/1jmi9+HPPdl61lO/9AflzSu/eLG82Tb+Tqvx2fLm3289Ud6MMcZfvvCj5c3bzr1W3mwbnxU3u8Z/SjbH9Q13nW8KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAOEg3mn1/Eut2YvHl8qbzbQqb77j3Cvlza1H1uXNGGP8wqc/UN6864nnyptnXn5zefPOR+o/p/+6/HB5M8YYP/vE58qbl44eKG8eOlM/xng41X+2i1u3yxvuPt8UAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAMJBvFNqcf5ca/fk+cvlzbOHj5c3LzYOrb314LXyZowxvnzpTeXNtc1BefOWB66VN1+6+lB58wOPPl/ejDHGzWmvvFmOXXmz2dUPJD554eXy5s+nqbzh7vNNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACAcxDuljl94cbZnXTpzs7x5296r5c355VF5M8YY33X2pfLm+lQ/iLdqHI/reGx9tbU7nNblzXpxXN5c3l4obzrm/B3n5HxTACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBcST2lvvCxH2ztrkz/UN7cblzfPFhsypvDXe8K6SNnrpc3D+7ql1+Xi6m8mXb1z1WHu/r73XW42ytvOtdin93Ur7F+4aPvKm/GGOPtv/KZ1o6T8U0BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIBzEm8Fzv/3u8uaLP/VHrWd95Mo7y5tLq/rxuM5Rt/WufjRtjDFenS6UN50jfxdXt8qbw8ZzloveYcBpt2jtqjqHAf/2699T3nzx5/6kvBljjO///IfKm8f/8B9bz7of+aYAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEA7izeDMjfrm69Nh61nXtwflzf5y03rWXPYW9UN606J+PG692JY3N8Z+ebOZep/FOgf7rm/Pljfrxvt9ZXO+vPnatvEPY4yxf7V3UJCT8U0BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIBzEm8Hho/UDXpvddBdeyf/3rNVMz+n9uq0W9fei+6z6c+rvXff9fqhxsK/zrGXj/V4u6r/jB4ve+3Bcv/lIgW8KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQrqTM4ftNxeXM445XU9UzXNzvXTscYY7urf3bZjkXjOfVN572bGn+fMXqvr2Oun+3hrv7ejTHG0aV53of7lW8KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAOEg3gwW6/qxsM1deB13Uueo23L0DuIdzfRrOjU+I3WP23V0Xt+yeYRwDoe7XWs37d3hF8L/4psCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQDiIN4PV3ra82fRuhY1V4wDaqnGobrNb1Z+zaB5Amxb1zUyH6jrvQ9d2xuN7c+i+c9uD5j8OTuTe+i0D4HURBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACAcxJvBwdmj8ub6tG49a9k4iLdtfDboPGe9OC5vxpjv9S0bhwG3o36sbzV6B906xw47hwHXi/oBx/3Gz/aweddu82D9feDkfFMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACAfxZnB2b1Pe3Nj1DuJ1j63Nofva5jrqtlrUX1/nta0ar22M3sG+zmHAjvWycxCv95l0t997/zgZ3xQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFdSZ3Bwpn5BcrO7934027Fo7TqXPjvXSzs6r637PnR+Jw4W9Qu9h80LvbPxUfau8vYCEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAxL13de0UurB3u7w52q1az+odj6sfaFuN3SybMXrH7TrvQ0fnta0X29azto3PcMtF4z1vTDrvw6b5mXS57r1/nIxvCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgDhIN4MDlab8maasde9Q2v1g3PbUT+81961DtUdlzed96F7rK9zuLCjc7iwc+Tv5rQub8YY4+DcUWvHyfimAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAO4tEyNQ7OnXabXf2fQ+cQXPu9a9zD67y+zoHEzmm7a9NBYzXGwV79wCQnd+/9ywagTRQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAcBBvBger4zf6Jdxxy8X0Rr+EU6FzcO528yBe71Bd/fWtRuNn2zjWd306Wx+NMS7uH7V2nIxvCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEK6kzOLvalDfbztnJMcbUuMA5jV15s17UL792XtsYYxws6u/fZlf/1d7ueu/5XDoXWQ936/Lm/PJ2eXNj2i9vVs1Luw/sH5Y39b/R/cs3BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwEG8G58/Uz3F1jsCNMcayeWSMntWY7/2eGgf75np9m92qvNk2DyQ+uHezvHmp9aT7k28KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAOEg3gwuH50vb7rHws4tj8qbzqG1bePzxJzH+raj/nfqPaf+PnRf22ZX/+d6blk/xric6Yje42dea+3+++rD5c2Fca31rPuRbwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4SDeDDoH56Zmr69uz5Y3N7f75c1j66vlzWvb+mHAMXqH4NaL7SzPmRqHCzuv7Ru74/Lm5lT/2a4Xq/Lm9rQubw539c0YY7xy5WJ5c6H1pPuTbwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCupM/jMp95R3vzpz3+i9awf2f9sebMZu/Lm1W398uvtXf365hhjXNvVL31uGs/qXC9djam82TY/iz24PCxvVo2f7WOr+t/pcFd/zvWp9z7s/0vv2i4n45sCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQCx2u5NdsnrP8n13+7VwByx++HvLm+d/4mJ5c+vR+gG07cX6wbkxxlicPS5v1gf1zbStf0ZaLOrvw9Q8BLd7uX4YcP9y/Vn7V8qT8dinr5c3u3+uH2/k9Xl6+vg3/TO+KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDEiQ/iAXDv800BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGA+B+lQEPg/iDf7wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in res:\n",
    "    image_= test_set[int(i)][\"image\"].permute(1, 2, 0).numpy()\n",
    "    plt.imshow(image_)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "abd37aeb-690c-4505-85ca-a5e2c0bd9ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9943/2220428050.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"clip.pt\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
    "model.load_state_dict(torch.load(\"clip.pt\", map_location=device))\n",
    "\n",
    "def top_products(query):\n",
    "    query_text, query_mask = tokenizer(query)\n",
    "    query_text = query_text.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    query_mask = query_mask.unsqueeze(0)\n",
    "    query_mask= query_mask.repeat(len(query_mask[0]), 1).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text_encoded= model.text_encoder(query_text, query_mask)\n",
    "\n",
    "        ## get the image \n",
    "        image_features= torch.cat([batch[\"image\"] for batch in test_dataloader], dim=0).to(device)\n",
    "        image_features= model.image_encoder(image_features)\n",
    "\n",
    "        \n",
    "\n",
    "        similarity= (text_encoded@ image_features.T)*torch.exp(model.temperature)\n",
    "        similarity= similarity.softmax(dim=-1)\n",
    "\n",
    "        _, top_indices= similarity[0].topk(5)\n",
    "\n",
    "        return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7116f7ad-44d8-4c5d-a8f0-683dc178604e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8499, 2230, 2867, 9840, 2907], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPV0lEQVR4nO3cu6/k91kG8Pc3lzNzrrvrS7Bsk9hxSATYQEBEBISAIhUNFOmpgAaJAiQa/gNq/gIalCINEqBI0CARhChsUAISInGcXV/2fnbPdW4UhLdD8fuSnOzOfj71PvrOfOd39plpnmGz2WwCACJi9KN+AQA8OZQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAGnycf/hl0Zf/mG+Dvg/3fndL5YzR+8uypmHr0/LmQc/tS5nPvcn/1rORESsT09bOfhfX1t/5fv+G78UAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgPSxB/Hg/+vst77Qyv31n/5ZOfMXD98qZ56bPC5nfufoo3LmV//+98qZiIi9r/5TKwcVfikAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIAySDelhlfv1bObF57pZx58JNH5czJy73vIL/x539czvz0b/5HOfNHr/xNOfPGX/5BOfOZr369nImIWP/a58uZ6Z3T+kHvvV+OrI6P6+fwRPJLAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYBkJfUKDL/4Vjnz+FN7rbMu9+s9P15sypn5vVU5M6zLkYiIGC3rr++d+efKmd9//7PlzCsf1O9h/BOfLmciIobTZTlz+eJ+/aAXP1OOjM/rr23y4cNyJiJi+a13Wzk+Hr8UAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgGQQr+iDP/zlcmZyWh90O7hVH1qLiJit6qtzy/lQzpy9MC5nht5ballcq9/D/tv1z+n2z9b/hJbzT5QzERHXvvGgnBnt1D+nWNfvYT2rn/P4zR8rZyIiZi9dL2eGf3y7ddazyC8FAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAID3Tg3ibX/m5eqaxLzY7rg+MLfZ7fT2/uyhnJqf1QbzFQf0iOsN7ERHTk/q43Yv/XH9959fqd/7Jv31czqynjYcoIpY3duuh+tVFND6mzagemt8+rx8UEcev75UzN/79Rjmzun+/nNkGfikAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIA6ZkexHv0qXk5szion3NxVB8LW09743HDZlrOzO90RvQ6S2u97yDDsn7W2Qv1e9i9Uz9nWNXHDoehc3cRo8Y9bBqP0VB/S7Ea1w9a7tU/o4je6xuuHdZDBvEAeNYpBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFANIzPYi3aozO7d+sr3EtDhvn3OqNpp1fr5+12mmM6N1blTPjy8aSWURMTpflzPTxTjmz2mmMuu3X7+7yWu/PbtS5v8bXvuW8Htr96LKcaWw3RkTEaFG/h+VL1+sHffs79cwW8EsBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgPRMr6Qud+uZvTv19dJ7P1M/Z7E3roci4oV/qy+KXlyrn9XJDL2R1Jic1B/TnZP6Ya2F2Xn9Hlaz3nexy4N6ZnLevPSi0UX9ubv7ZuMNRcThe/WF3tVu/Rnq/QU+/fxSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFANIzPYgX9f2z2Dmuj3FtdurdO/qFh+VMRMTwzn45c3lYv4jdu42hteY227pxf+vGmtn4op6JTf1NTU7ro4oREbPz+rM3vqifNXl4Vs6s93bKmZNXy5GIiHjh7foHtTiqvz6DeAA885QCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIAaWsG8cZHR+XMZtRYxFvXB9Dmt+rXfBL1YbuIiBvT+nvaNJa/No2vE+OL3iLe+XP1F7ieND7bK7Ka917bal5/jkaL+p2PH1+WM+tp4zOaliP/Y6jf32pWz4z263+D65OTcuZJ45cCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkLZmEG84OqyHGvtso1U9tJ41Dhp643Gd0bn15GoG52b3V+VMRMRir/6YDuv6OZ2Rv/W0HuoOA3aGC5e79dc3Oq8P4j3+9EE5s97p3UNH586Hg8YopUE8ALaJUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQDS1qykRmPps1OJo/P60udqXl9onBwuypmIiNFF4x7qg6cxWnbWYhsHRXOZdtw4qxFZT+uhnYfL+kERsTiqf7adtdhY1UMnL9Vf2+jy6lZSO4bJ9vz3WOGXAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJC2ZvFp8fKNcmZojLptJvUeXTdueXfWG8Rbz6blzKYxBDc5r9/dUN8SjIjeuN1qVj+n9Z7WnWeoNwzYub/W69vdKWcObtVf3ONP9b6Trub18b1VY4xx/fxRORM3b9UzTxi/FABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYC0NYN4l9fqI16T8/o5651Gjw71UbKLbx3Wz4mIi2v1zGhZzwyr+ntaT3tDcKv6R9se36vaNMb6OkOMERHDtDFuN6o/r53Rx/mdy3Jm+GTvQ5oe1+/h0Y8flDOze7vlzDZ8y96G9wDAD4hSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAIG3NIF6r3hq7ZKt5/aDxWT3z6t8typmIiOPXpvVQb5+tbD3pDeJ1PtvJaf1NXR7VX9+4vgMXw6Y5iNfYj9t0rrwxojes1uXM4t68nImIWB7UFxw3jWeoM365Dd+yt+E9APADohQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABIWzOIt5rV+20zbpzTGMma366vku1+91E5ExHx4ReeL2emx/VzRpf1UbfVrH5ORLQG+4bO2OGs/jmtx1e0JhgRk5P6ENzFtfpDvjyojyrOvnmznDn4rzfKmYiIh6/V39P4ov45jc8bC4RbwC8FAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFANLWrKQO6/oK4mhRP2exV1/SPPzuun7Qe+/XMxFx/tL1cmb3w/p3g9GqsZK6U45ERMTQGKtcNxZwd44bz1DjtS13Gy8uInYe1h/YzlrsYr/+38L4gw/LmclZbyV1cVj/Gxzfq1/E4rC+Ftt8xJ8ofikAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIAaWsG8TpGy/pI1nJeH+M6+PZFObM6Pi5nIiI2k/r43mjR+G7QGCBczup3FxExvqxnOkN1F9frr2/+oHHQFeo845sr+l9h09sFjHV9py6mJ4176D2uTz2/FABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYC0NYN4m6G+XrWe1jOrWTkSk+P6IF59vut7xvXk+LKeWe3W18y6Q2vDeeM2GpHlfiMzr3+vmp0t6wdFRGyu5h42o6tZgps+7j3lx6/XM6NFYxBv/Gwu4vmlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKAKStGcTr2DQqcTWrj2SN7z4qZ5qTaRGL+psaGrtkq52rGSCMiBg3xsxWO41zzuqZjs2kdw+bceezbdzdtHHOpP5fyXPfOClnIiLuvbVXDzWufFi2Zymfan4pAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAGlrBvE6w1/DunFOI7N6/8N6aDSuZyJidF7v+VFjfa8zJrjp7cDF5Kx+6Q/eqD/aux/Vn6FRY6xvMzQvonHnQ+eznTfO2d2tn/P1d+oHRcTot79Yzpw9X/972vuoPUv5VPNLAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYC0PSupq06mvnC5ntYXLjeL+triMO19NJtx4z2N6+9paEyebppP26YxGDt7UL+HxVH9Pe3dvZpnKCIiGs/raFFfmL3cr1/4ML6675eref0e5g/q99D+nJ5yfikAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIAaWsG8TqjaYv9+uDVqL5tF7FurPWtG28o4spqftUYCxs6dxcRUd8/i51H9dD9l+vn7N6uX/jO4/o4W0TEZlo/a2jcXesZn+40Qj2TT5yVM6vpvJyZ3e8+sE83vxQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGAtDWDeJPT+sjYZlwfnWuPuhWNPvt6L3hQf4HTs/o9nL5Yz4yWjXW2iBhd1nNnL9a/7+zfqp/TGbdb7fS+iy3n9TsfX9RfX+dzGg72ypm4XY9ERCwe1cf3lruN5+GmQTwAnnFKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgLQ1g3jr6VDOrGb1TPQ23cpW3/zPVm6z/nw5s9hrfDdo3MNQ32b7XrARaWyZnT9fP2jvo3pmtGg+RI2PaVjXzxo17m5z93491LT73Fk5s5wf1jN79f8ep+XEk8cvBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQDS1qyk7ty/LGcmp/W3v/Oovjo5TOrnbJaNqcqIePWvxuXMB79UX/o8eLeeGV8010EbK6l7t+uTrKtZ/TvS0HhLk9NVPRQR62n99V08V9/tXM4bi8PHx+VM1/AvR/XMqv5BTc56n9PTzi8FAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAIA2bzeZjLUV9afTlH/Zr4Udk9es/X87cfXNezqyb84uT08YIYX0PL47fqGf2b9bH43YbY30REaNl/R7Wk/rru/EP3ylnljdvlTNcva+tv/J9/41fCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAED62IN4AGw/vxQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEj/DWyCyLgtbxyBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPTUlEQVR4nO3cS4sl93kG8Lfq3Po203ORPJEtjYOtGCwc24EQCASyMPoICmSbb5DPkmV22WofHLTxIighGKKNRYhy0V0jaa49092nz6UqC8MLgQTN+080Gp3+/dbz8K+uU2eeU5unG8dxDACIiP6bvgAAnh9KAYCkFABISgGApBQASEoBgKQUAEhKAYA0fdp/+Hr/xtd5HfC/mrz2o3Kme3xWzmw++ricgW+Tt4Y3v/LfeFMAICkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUA0lMP4sH/1d99+k5T7sH27XLm3fVeOfODaX1E7/HYlTN/+ad/Xs5ERGze/7ApBxXeFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYBkEI+YXDsuZzY//t1y5tVf/byciYj4i5/8Qznz9/d+WM78zatvljN/ffdPypnhzhflTEREf3hYznSTSTmzPTkpZ9gd3hQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASN04juPT/MPX+ze+7mvZWd20PkY7uf1y01nDwV49czQvZyaPzsuZ8+9fK2ciIqanm3Lm3/+s/jdd/df6ouitv3q7nJm+0vbZjvuLcmY4PihnuvN1PbOqZ8ZP7pQzERHD6WlTjoi3hq9eAvamAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKAKT6Utsl1zJmtvzRrfpBJ6t6JiLGSVcPDU+1ifjf9fXfE/sfPKyfExHnt4/LmRf/qX59N//2X8qZ7R/9fjkTn96vZyIihqEc6R88qZ8zn5Ujw2HDEONPXy1nIiKmv/nPcmZ7ctJ01mXkTQGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABIBvGKnvzsu+VMv64PmW0Xk3ImImJysa1n7j0uZ9YvXStnpp8/KmciIuaP6uOAw7zh986tF8qRycOz+jkNg3MREd2m/tmO04bnqOGcybL+GY2Lq+VMRMTww1fqoX/+TdNZl5E3BQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACBd6kG86cvfK2fGhn2x6aP6wNjQOIjXYvxOfZhs8mhZP2gxr2ciYvLovJxZ/t5hOXN472E5M754o5yJoT6QGBHRNcS6YSxnxqgfNO7VP9vJScMzFBHDomFQsG/4Pg317+0u8KYAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoApEs9iDdeOShnuoaNrO2i3r3Lmw2jXxGxf7cpVtbNGgbG1m0DY92mnuu39SG4bt4w2DfWz2nWcNbYd/Vz+obfiutNPTNtG30cG8YiJ9ePy5ntvfvlzC7wpgBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAutQrqdE1LEg2RPber68t3n/tpfpBEdEN9Y/0yvurcmZ7tb4o2p83LGlGxGR5Uc7Mzob6QQ2LosNew1fooGGNNSL60/p96M6W5cx40LDQ27Bku7lxWD8nIrqWtd2+bZH1MvKmAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKAKRLPYi3PVqUM912rGeenJUzF9fLkYiI6Ff1nj+4U38MutWzGZyLiBiP9suZrmF7bzysnxN9w++qoeHeRcQ4q4+6dS3X1zJud/OonBnbHoeYnK3LmW7RNkJ4GXlTACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFANKlHsTrV/Xhr2FWX/EaHj8pZzaH9eG9iIj11fr1jdN6ZnJWX5xbH++VMxER82X9rJbhwiYt43Zte3jRberBcTGrn/PgpJzZvnKtnJndX5YzERFdyz2f1+/DZeVNAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEiXehBvc7woZ2aP6yN6LTY310251UV9+Gv68KKc2R7Ny5lubBypaxiC61qOWjXc85bBucb7MMzrX9fJlw/r59y6Uc5s5/Xfl9PFpJyJiIiL+uc0Tvz+fVruFABJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBpd1ZS+/ri4vkL9aXPgzv1RdFuUr+2ftG2xrp6od7z3VBfIW3RDW3roNvjvXJm+qS+pNlt6ve8WzdkGldSx4aV1O2dz8uZ+7+4Xc5s9suR+J0PT+qhRuOi/l2/rLwpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAGlnBvEmV4/KmcWDTf2ghlG38fZ3y5nrv6qPwEVE3PvDhiG9ritHtov674lxUj8nImJx97ycWd46KGdm81k5023bhgtb9Bf1kb+4cqUe+WhVzlzcqP9Xcvb9q+VMRMTBv92vh+YNv38bvhfROHb4PPGmAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKAKSdGcTrGoa/omHvqt8M5czq1mE58+KvH5UzERFnt66VM8Ne/THo6rchutaxsIazZqcNY4cN1zfszcuZyaPTciYiYpzUf8N1R/Vnb++9z8uZ2QvH5czZ7fq1RUQMx/Wxw2j43nbz+mc7XlyUM88bbwoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBA2plBvLFhmKy/2JYz3ao+tLbZ3y9nFh98Vs5EROzfrQ+TLV/cK2dmT+r3YXLWMFIXEesbDdd3f1k/qKsvJHZDfWitZdjut8H6YN94pWE87rMvy5G+5fu3bri2iOjW9e/t6kbDd7BhTHBrEA+AXaIUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASLsziLeoD3KNs3ondmcNg1fdlXJk++BB/ZyImJ3WR9P6dT3TbeqZGBoyETE7WZUzZ7frY2ZH756XM915/dpiOqlnImKc1HPdal3PXK+PKm4//KSciVdv1jMRcfFCw7jdl2flTHfQMNh3734985zxpgBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBA2p2V1FnDguR6qB/UdeXIMKtnWk2X9SXSi2v1ezd/WF+LHRZtj9v0ZFnO9KuGRdaxnmlZ5+0uGpZVI6JrePZa/qY4r9/v8aL+PIyTtu/F9GRbznSn9b8pNpt6Zgd4UwAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQDSzgzibQ9n5Uy3rY+FbW4elTPT0/qAV6t+U/+bhoZhsn5ZHwsbJ22/QdY39uuhhq21bmgYxJs2/E2rxoHETf05Go7q926ybBvsq9rst92H7d6inDn+oP43jdeulDPx2Z165jnjTQGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABIOzOIN8wn5cz0ScNIVsOo22Q1lDOt+ov6qNtiWR9a2x7Oy5n+bF3ORET0q/o9P32pfn2H5UREt6oPA0bjMGD09Vzfcn1942Bf0dFH5025kx8clDPjlXqmu2h7Xr/tvCkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIAaWcG8baLhrGwdf3P74b64NzswbKcaZ3QGxb1MbNNw72bP7woZ8ZZ22+Q7eGsnDn8rGHssGEIbnt1r5yZPK7fu2dpPNx/JucM07bnYX1Q/5xaxu1axi93weX8qwH4HykFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUA0s4M4rUMwXUP67Nz47zeo916W848U/VbF/15fWBsfaNtaK1b1z+n7d6kfs6m/jlNThrGDhf1gb+Ixudo0vC9WG3q5zSYf3C3Kbf5g9v10NDwXT+ojx3uAm8KACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKAKSdWUld3F2VM0PDkma/qq8txjjWM42GacMq5rZ+fZur9QXJ6clFORMREQ23/N5P6ous++81fE5d/X73y/qzGhExHNXvedewZvusbD7+pCm39+DlcmZ7/Uo5M7lzr5xpeFSfO94UAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgLQzg3izO4/Kmcc//U45s/dlfcysOzktZ1oNk/pA2/qwnjn8tGE8rnEtbHNtUc7MWm75UP+b1tfrw3vzTx+WMxER/bZ+A8f5rJ5Z1DNNGocix77+vI6z+u/fcdk44Pgt500BgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASDsziNc9OatnGgbQmka8ttt65hnq1/VM1zDONhy0Da1NzjflzOp4r37Qpn5Ov2m4D1cPypmIiO6sPtDWNTyvbTN1z06/rl/hsKj/Vzfdqw8x7gJvCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEDamUG86LpyZHk8KWcm5/VbNrt+tZyJz+7UMxFx/R8/KWe++MXLTWdVTT9/1JTrNvVBweXNo3JmPD8vZ4ZZ/RnqzxsWCCMipvWz1tf36+dM6t+lZ/nrsqtvEMbmoH7v5tPd+e+xwpsCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAKkbx3F8mn/4ev/G130tz15fX06Mob7Y2bLgGk/3sfy/6H/+Wjnz+R8flzOTZTkSERHbvXrm9Hv1zO1f1ldSV9fm5cxmv+F5iIi+4dE7+PisHvr1u/VMy/eCZ+6t4c2v/DfeFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYA0/aYv4Bv1rEa8nuG4XYvhnfoA2ovv1M/pf/bjeigihv1ZOXPr7XU5sz2oj9vtfVEf0Zs8bBipi4jte/9RDz3nzx7PH28KACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQOrG0WIWAL/lTQGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgPRfp8HSJhAAvScAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res= top_products(\"shirt\")\n",
    "\n",
    "print(res)\n",
    "for i in res:\n",
    "    image_= test_set[int(i)][\"image\"].permute(1, 2, 0).numpy()\n",
    "    plt.imshow(image_)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0010802-4e7c-4ed5-b13e-a333bbf2cf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 32])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features= []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    images= batch[\"image\"].to(device)\n",
    "    features= model.image_encoder(images)\n",
    "    image_features.append(features)\n",
    "\n",
    "image_features= torch.cat(image_features, dim=0)\n",
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f10924f0-ab94-4404-8528-2144ddd022bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 32])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features= torch.cat([batch[\"image\"] for batch in test_dataloader], dim=0)\n",
    "image_features= model.image_encoder(image_features.to(device))\n",
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a066bd-66e0-4deb-9a9c-6c682aca2634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
